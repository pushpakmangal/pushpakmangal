{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvCrJwQlRdXn6BXChey1D3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushpakmangal/pushpakmangal/blob/main/id_NIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objective:\n",
        "To create a model that summarizes the details on any government ID's . It generates the\n",
        "textual information and the picture of that government ID in a summarized manner.\n",
        "\n",
        "##Approach:\n",
        "1. Data Collection and pre-processing:\n",
        "a) Gather a large dataset of government ID images and their corresponding textual\n",
        "information. Ensure that the data is diverse and representative of various ID types and\n",
        "formats. b) Preprocess the images and textual data to prepare them for the training process. This\n",
        "includes resizing images to a consistent size, converting textual data into numerical\n",
        "representations, and normalizing the data. Eg for pre-processing code.\n",
        "2. Building the Model:\n",
        "a) Use TensorFlow to create a deep learning model that combines computer vision and\n",
        "natural language processing components. b) For the image processing part, consider using convolutional neural networks (CNNs) to\n",
        "extract features from the ID images. We can use pre-trained CNN architectures like VGG, ResNet, or custom-designed CNN layers. c) For the text processing part, we can use techniques like word embeddings (e.g., Word2Vec, GloVe) to convert words into numerical representations and feed them through recurrent\n",
        "neural networks (RNNs) or transformer-based models like BERT. d) Merge the outputs from the image and text processing components to create a joint\n",
        "representation.\n",
        "3. Loss Function and Optimization:\n",
        "a) Define an appropriate loss function that takes into account both the image and textual\n",
        "components of the model. b) Use optimization algorithms like stochastic gradient descent (SGD), Adam, or RMSprop to\n",
        "train the model.\n",
        "4. Training:\n",
        "a) Split your dataset into training and validation sets. b) Train the model on the training set and monitor its performance on the validation set. Adjust hyperparameters as needed.\n",
        "5. Evaluation:\n",
        "a) Evaluate the model on a separate test dataset to assess its performance in generating\n",
        "accurate summaries.\n",
        "6. Deployment:\n",
        "a) Once the model is trained and evaluated, you can deploy it in a production environment\n",
        "where it can take an input image of a government ID and provide both textual information\n",
        "and a summarized image.\n",
        "\n",
        "##Sample Code:\n",
        "\n",
        "Note: This code is written for demonstration purpose, to get the intuition behind the task."
      ],
      "metadata": {
        "id": "vZO7ghoEHXKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Dummy data\n",
        "# In real world, it is replaced with actual ID images and corresponding texts.\n",
        "images = tf.random.normal((100, 224, 224, 3))\n",
        "texts = [\"hello\", \"world\", \"tensorflow\", \"example\", \"text\", \"extraction\"] * 20\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Read the image from the provided file path\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Gaussian blur to the grayscale image\n",
        "    blurred_image = cv2.GaussianBlur(gray_image, (3, 3), 0)\n",
        "\n",
        "    # Apply thresholding to make the text more prominent\n",
        "    _, threshold_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    return threshold_image\n",
        "\n",
        "#purpose of this callback is to monitor the training progress and halt the training process if a certain condition is met\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    '''\n",
        "    Halts the training after reaching 60 percent accuracy\n",
        "\n",
        "    Args:\n",
        "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
        "      logs (dict) - metric results from the training epoch\n",
        "    '''\n",
        "\n",
        "    # Check accuracy\n",
        "    if(logs.get('loss') < 0.1):\n",
        "\n",
        "      # Stop if threshold is met\n",
        "      print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "# Instantiate class\n",
        "callbacks = myCallback()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Below code is using the ImageDataGenerator class from Keras to generate augmented image data for training a machine learning model.\n",
        "The ImageDataGenerator class is a powerful tool that allows you to perform various data augmentation techniques on the fly during training,\n",
        "which helps in improving model generalization and robustness.\n",
        "After defining the augmentation and validation generators using ImageDataGenerator,\n",
        "the code creates two data generators: train_data_gen and val_data_gen.\n",
        "By using ImageDataGenerator with various augmentation techniques, the model is exposed to a wider range of data during training,\n",
        "which helps prevent overfitting and improves its ability to generalize to unseen data.\n",
        "It is a common practice to use data augmentation when training deep learning models, especially with limited training data\n",
        "\n",
        "augmented_image_gen = ImageDataGenerator(\n",
        "    rescale = 1/255.0,\n",
        "    rotation_range=2,\n",
        "    width_shift_range=.1,\n",
        "    height_shift_range=.1,\n",
        "    zoom_range=0.1,\n",
        "    shear_range=2,\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    validation_split=0.2,\n",
        "   )\n",
        "\n",
        "normal_image_gen = ImageDataGenerator(\n",
        "    rescale = 1/255.0,\n",
        "    validation_split=0.2,\n",
        "   )\n",
        "\n",
        "train_data_gen = augmented_image_gen.flow_from_directory(batch_size=batch_size,\n",
        "      directory=\"tensorflow_training_images\",\n",
        "      shuffle=True,\n",
        "      target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "      class_mode=\"categorical\",\n",
        "      subset='training')\n",
        "val_data_gen = normal_image_gen.flow_from_directory(batch_size=batch_size,\n",
        "      directory=\"tensorflow_training_images\",\n",
        "      shuffle=True,\n",
        "      target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "      class_mode=\"categorical\",\n",
        "      subset='validation')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create a vocabulary from all unique characters in the texts\n",
        "vocab = sorted(set(\"\".join(texts)))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create a mapping from characters to indices and vice versa\n",
        "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "# Convert texts to numerical sequences\n",
        "sequences = [[char_to_idx[char] for char in text] for text in texts]\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences, maxlen=max_seq_length, padding='post'\n",
        ")\n",
        "\n",
        "# Model architecture\n",
        "input_shape = (224, 224, 3)\n",
        "num_units = 128\n",
        "\n",
        "# CNN for image processing\n",
        "model_cnn = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(num_units, activation='relu')\n",
        "])\n",
        "\n",
        "# RNN for text decoding\n",
        "model_rnn = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, num_units, input_length=max_seq_length),\n",
        "    tf.keras.layers.LSTM(num_units),\n",
        "    tf.keras.layers.Dense(num_units, activation='relu')\n",
        "])\n",
        "\n",
        "# Combine CNN and RNN outputs\n",
        "combined_output = tf.keras.layers.concatenate([model_cnn.output, model_rnn.output])\n",
        "output = tf.keras.layers.Dense(vocab_size, activation='softmax')(combined_output)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.models.Model(inputs=[model_cnn.input, model_rnn.input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.SGD(lr=0.01)\n",
        "model.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['categorical_accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x=[images, padded_sequences], y=padded_sequences[:, 1:], batch_size=32, epochs=150, callbacks=[callbacks])\n",
        "\n",
        "# Function to predict text from an image\n",
        "def extract_text_from_image(image_array):\n",
        "    # Preprocess the image (if required)\n",
        "    # image_array = preprocess_image(image_path)\n",
        "\n",
        "    # Convert image_array to batch format\n",
        "    image_array = tf.expand_dims(image_array, 0)\n",
        "\n",
        "    # Get the CNN output for the image\n",
        "    cnn_output = model.get_layer('dense').output\n",
        "\n",
        "    # Create a dummy sequence for the decoder\n",
        "    start_token = tf.constant([[char_to_idx['^']]])\n",
        "\n",
        "    # Initialize the output sequence\n",
        "    output_sequence = start_token\n",
        "\n",
        "    # Maximum length of the predicted sequence\n",
        "    max_length = 100\n",
        "\n",
        "    # Predict the text character by character using the RNN\n",
        "    for _ in range(max_length):\n",
        "        prediction = model.predict([image_array, output_sequence])\n",
        "        predicted_char_index = tf.argmax(prediction, axis=-1)[:, -1]\n",
        "        output_sequence = tf.concat([output_sequence, predicted_char_index], axis=-1)\n",
        "\n",
        "        # Stop if the end token is predicted\n",
        "        if idx_to_char[predicted_char_index.numpy()[0, -1]] == '$':\n",
        "            break\n",
        "\n",
        "    # Convert the numerical sequence back to text\n",
        "    predicted_text = \"\".join([idx_to_char[idx] for idx in output_sequence.numpy()[0]])\n",
        "\n",
        "    # Remove start and end tokens and return the predicted text\n",
        "    return predicted_text.replace('^', '').replace('$', '')\n",
        "\n",
        "# Example usage:\n",
        "image_path = 'image_path.jpg'\n",
        "image_array = preprocess_image(image_path)\n",
        "extracted_text = extract_text_from_image(image_array)\n",
        "print(\"Extracted Text:\")\n",
        "print(extracted_text)\n"
      ],
      "metadata": {
        "id": "xiGtV6i_HZVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}